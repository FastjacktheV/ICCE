<!DOCTYPE html>
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<title> C/C++ Exercises </title>
<style type="text/css">
    figure {text-align: center;}
    img {vertical-align: center;}
    .XXfc {margin-left:auto;margin-right:auto;}
    .XXtc {text-align: center;}
    .XXtl {text-align: left;}
    .XXtr {text-align: right;}
    .XXvt {vertical-align: top;}
    .XXvb {vertical-align: bottom;}
</style>
</head>
<body>
<hr>
<ul>
    <li> <a href="https://www.icce.rug.nl/edu/2/opgaven.html">Table of Contents</a>
    </li><li> <a href="https://www.icce.rug.nl/edu/2/opgaven07.html">Previous Chapter</a>
</li></ul>
<hr>
<a name="l8"></a>
<h1>Chapter 8: Exercises set eight: Multi-threading II. Extended deadline: Feb 15,
                                                                    2018</h1>
            <br><br>
<p>
</p><hr>
        <strong>Exercise 57.</strong>
        <br>
        Purpose of this exercise: learn to design and implement a Semaphore class
<p>
Design and implement the class Semaphore, offering the following facilities:
    </p><ul>
    <li> <code>Semaphore(size_t nAvailable)</code><br>
       With a counting semaphore <code>nAvailable</code> defines, e.g., the number of
        available locations in a storage area.  Locking/unlocking, supporting
        facilities to notify other waiting threads is implemented via binary
        semaphores, which are initialized to 1 or 0.  A semaphore containing
        the value 0 blocks. I.e., its wait member waits until its value is
        incremented by another thread, calling one of the semaphore's notify
        members.
<p>
</p></li><li> <code>void notify()</code><br>
       The internally maintained <code>d_nAvailable</code> value is incremented and one
        waiting thread (cf. the wait members below) is notified, reactivating
        that thread.
<p>
</p></li><li> <code>void notify_all()</code><br>
       The internally maintained <code>d_nAvailable</code> value is incremented and all
        waiting threads (cf. the <code>wait</code> member below) are notified. Only one
        waiting thread will be able to obtain the semaphore's lock and to
        reduce available, and that particular thread is thereupon reactivated.
<p>
</p></li><li> <code>size() const</code><br>
       Without trying to lock the Semaphore object's mutex the current value
        of the Semaphore's <code>d_nAvailable</code> value is returned.
<p>
</p></li><li> <code>void wait()</code><br>
       This member blocks for as long as its internal variable
        (<code>d_available</code>) equals zero.  When returning from <code>wait</code> the
        current thread holds the lock of the <code>std::mutex</code> which is
        maintained by the Semaphore object. The <code>notify</code> members are used to
        increment <code>d_available</code> and to inform <code>wait</code> that it may return.
        When multiple threads are waiting only one thread will stop waiting,
        while the remaining threads will continue to wait for another
        notification.
    </li></ul>
This class is used in subsequent exercises. If you do not complete this
exercise then you may assume that <code>Semaphore</code> is available in those
exercises. 
<p>
<br><strong>Answer:</strong><br>
    Here is the class header:
    </p><pre>#ifndef INCLUDED_SEMAPHORE_
#define INCLUDED_SEMAPHORE_

#include &lt;functional&gt;
#include &lt;mutex&gt;
#include &lt;condition_variable&gt;

class Semaphore
{
    mutable std::mutex d_mutex;
    std::condition_variable d_condition;
    size_t d_nAvailable;

    public:
        Semaphore(size_t nAvailable);

        bool wait( std::function &lt;bool(size_t &amp;)&gt;  fun);
        void notify_all();  

    private:
};

        
#endif
</pre>

<p>
The constructor:
    </p><pre>#include "semaphore.ih"

Semaphore::Semaphore(size_t nAvailable)
:
    d_nAvailable(nAvailable)
{}
</pre>

<p>
The member <code>notify_all</code>:
    </p><pre>#include "semaphore.ih"

void Semaphore::notify_all()
{
    lock_guard&lt;mutex&gt; lk(d_mutex);  // get the lock
    if (++d_nAvailable == 1)
        d_condition.notify_all();   // use notify_one to notify one other
                                    // thread
}
</pre>

<p>
The member <code>wait</code>:
    </p><pre>#include "semaphore.ih"

bool Semaphore::wait(std::function&lt;bool(size_t &amp;)&gt; fun)
{
    std::unique_lock&lt;std::mutex&gt; lk(d_mutex);   // get the lock

    while (d_nAvailable == 0)
        d_condition.wait(lk);   

    if (d_nAvailable == 1 &amp;&amp; not fun(d_nAvailable))
        return false;

    --d_nAvailable;                             // dec. semaphore

    return true;
}
</pre>

<p>
The member <code>size</code> simply returns <code>d_nAvailable</code>
</p><p>
<br><br>
    
</p><p>
</p><hr>
        <strong>Exercise 58.</strong>
        <br>
        Purpose of this exercise: become familiar with <code>packaged_task</code>.
<p>
Let's assume we have two matrices we would like to multiply. To multiply
matrices the inner products (cf. <code>stl::inner_product</code> of the rows of the lhs
matrix and the columns of the rhs matrix are computed, and the computed values
are stored in the corresponding elements of the resulting matrix.
</p><p>
As 2-dimensional matrices are stored by rows, accessing the elements of
columns is relatively hard, so let's assume the rhs matrix has been
transposed: the rows of that matrix then represent the colums of the rhs
matrix, and by computing the inner product of row x of matrix lhs and <em>row</em>
y of the transposed rhs matrix we obtain the value of element <code>[x][y]</code> of
the resulting matrix.
</p><p>
In this exercise simply define
    </p><pre>
    double lhs[4][5];
    double rhsT[6][5];        // consider this the transposed rhs
    
</pre>

<p>
Fill these matrices with values (don't use the default available values as
they may be NaNs, but other than that use whatever you like to fill the
matrices with). 
</p><p>
Then, use <code>packaged_tasks</code> to compute the 4 x 6 = 24 inner products (hint:
define <code>future&lt;double&gt; fut[4][6]</code>) in separate (detached) threads.
</p><p>
Finally, display the results in 4 rows of each 6 blank separated values, each
row on a separate line.
</p><p>
This program doesn't require much code: all of its code may be defined in one
source file.
</p><p>
<br><strong>Answer:</strong><br>
    </p><pre>#include &lt;iostream&gt;
#include &lt;future&gt;
#include &lt;numeric&gt;
#include &lt;algorithm&gt;
#include &lt;mutex&gt;
#include &lt;iomanip&gt;

using namespace std;

mutex out;

// std::inner_product provides the required function

double ip(double *lhs, double *rhs, size_t row, size_t col)
{
    row = row * 10 + col;
    out.lock();
    cerr &lt;&lt; setw(2) &lt;&lt; row  &lt;&lt; '\n';
    out.unlock();
    return inner_product(lhs, lhs + 5, rhs, 0);
};

typedef packaged_task&lt;double (double *, double *, size_t, size_t)&gt; Task;

int main()
try
{
    cerr.fill('0');

    double m1[4][5];
    double m2[6][5];        // consider this matrix transposed

    fill_n(&amp;m1[0][0], 20, 1.);
    fill_n(&amp;m2[0][0], 30, 2.);

    future&lt;double&gt; fut[4][6];

    for (size_t row = 0; row != 4; ++row)
    for (size_t col = 0; col != 6; ++col)
    {
        Task task{ip};

        fut[row][col] = task.get_future();
        thread{move(task), m1[row], m2[col], row, col}.detach();
    }

    for (size_t row = 0; row != 4; ++row)
    {
        for (size_t col = 0; col != 6; ++col)
            cout &lt;&lt; fut[row][col].get() &lt;&lt; ' ';
        cout &lt;&lt; '\n';
    }
}
catch (...)
{
    return 1;
}






</pre>

<p>
<br><br>
    
</p><hr>
        <strong>Exercise 59.</strong>
        <br>
        (optional)<br>Purpose of this exercise: become familiar with <code>packaged_task</code> (2).
<p>
Same exercise as the previous one, but this time use a worker-pool of 8
separate (detached) threads to execute the tasks of the packaged tasks.
</p><p>
Hints: 
    </p><ul>
    <li> pay close attention to the information in the Annotations about
packaged tasks: they are prepared by one task, but executed by another.
<p>
</p></li><li> Since the worker-pool is detached you must find a way to end the
detached threads lest the detached tasks keep running and thus your program
won't end.<br>
One way to realize this is to send the workers information that they can
stop. For that the standard communication channel that's used to send the
workers the information about which matrix element to compute can be used.
    </li></ul>
<p>
Once the results of all computations are available, the computed matrix can be
displayed. 
</p><p>
<br><strong>Answer:</strong><br>
    The basic setup for a multi-threaded worker pool is shown below.  This
example uses the value -1 to inform the workers that they can stop
working. Note that -1 remains on the queue, so other workers see the same
info:
</p><p>
</p><pre>#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;queue&gt;

#include &lt;bobcat/semaphore&gt;

using namespace std;
using namespace FBB;

enum
{
    TODO     = 10,
    NTHREADS = 8
};

queue&lt;int&gt; taskData;                        // -1 indicates end
mutex qMutex;

Semaphore producer{NTHREADS};
Semaphore worker{0};

int getSpecs()
{
    lock_guard&lt;mutex&gt; lg(qMutex);

    int ret = taskData.front(); // see what's in store
    if (ret == -1)              // -1: then done
        worker.notify_all();    // inform the other workers as well
    else
        taskData.pop();         // or use this request.

    return ret;
}

void client()
{
    while (true)
    {
        worker.wait();              // wait for task specs

        if (getSpecs() == -1)       // all done
            return;

        // insert any additional processing here

        producer.notify();          // ready for the next task
    }
}

void push(int value)
{
    lock_guard&lt;mutex&gt; lg(qMutex);
    taskData.push(value);   // push the task spec.

    worker.notify_all();    // notify a worker (plain 'notify' would be OK for
                            // the final -1 value)
}

int main()
{
                                //  start the threads.
    for (size_t count = 0; count != NTHREADS; ++count)
        thread{ client }.detach();

                                // prepare the tasks
    for (size_t idx = 0; idx != TODO; ++idx)
    {
        producer.wait();
        push(idx);
    }       

    push(-1);                   // all specs were pushed: 'end of task'

    cout &lt;&lt; "DONE\n";
}






</pre>

<p>
The next example shows the use of packaged tasks (pts): a series of 10 pts
are used performing some plain tasks. Note the comment: at pt-construction
time specific function could easily have been associated with the pts, without
having to change the worker threads.
</p><p>
</p><pre>#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;queue&gt;
#include &lt;future&gt;

#include &lt;bobcat/semaphore&gt;

using namespace std;
using namespace FBB;

queue&lt;int&gt; taskData;            // -1 indicates end
mutex qMutex;

size_t nToDo = 10;              // # values to process
size_t nWorkers = 8;

Semaphore producer{nWorkers};   // semantics: using 'producer' as the name
Semaphore worker{0};            // for the producer semaphore, telling the
                                // producer how many workers are available,
                                // and 'worker' for the workers semaphore, as
                                // this results in `producer.wait' in the
                                // producer, and 'worker.wait' in the worker
                                // functions. 

double ip()                     // function to do by the P-tasks.
{
    return 12.4;
};

                                // now that the function is known, define a
                                // matching typedef for the P-task.
typedef packaged_task&lt;double ()&gt; PTask;

PTask pTask[10];                // These P-tasks produce the results

int getSpecs()
{
    lock_guard&lt;mutex&gt; lg(qMutex);

    int ret = taskData.front(); // see what's in store
    if (ret == -1)              // -1: done
        worker.notify_all();    // inform the other workers as well
    else
        taskData.pop();         // or use this request.

    return ret;
}

void workerFun()
{
    while (true)
    {
        worker.wait();          // wait for the task specs

        int spec = getSpecs();  // get the specs

        if (spec == -1)         // all done.
            return;

        pTask[spec]();          // otherwise perform the task

        producer.notify();      // ready for the next task.
    }
}

void produce()
{
                                // # of tasks to do
    for (size_t idx = 0; idx != nToDo; ++idx)
    {
        pTask[idx] = PTask{ip}; // create the matching P-task. Note that the
                                // tasks could be different, conditional to
                                // the task index. E.g., returning a negative
                                // value for odd numbers:
                                //  PTask{ idx &amp; 1 ? ipNeg : ip }

        producer.wait();        // wait for available worker

        qMutex.lock();          // push the task specs
        taskData.push(idx);
        qMutex.unlock();

        worker.notify_all();    // allow a worker to process it.
    }

    qMutex.lock();              // all specs were pushed
    taskData.push(-1);          // -1 indicates `end of tasks'
    qMutex.unlock();

    worker.notify();            // notify a worker: that worker will
                                // automatically notify the others
}

int main()
{
                                //  start the threads.
    for (size_t count = 0; count != nWorkers; ++count)
        thread{ workerFun }.detach();

    produce();                  // produce the results
        
                                // show the results.
    for (size_t idx = 0; idx != nToDo; ++idx)
        cout &lt;&lt; pTask[idx].get_future().get() &lt;&lt; ' ';

    cout &lt;&lt; '\n';
}






</pre>

<p>
Finally, the matrix multiplication program. The same setup as the previous
example. We're assuming that the rhs-matrix has been transposed, so the
standard <code>inner_product</code> algorithm could be used:
</p><p>
</p><pre>#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;queue&gt;
#include &lt;future&gt;
#include &lt;algorithm&gt;
#include &lt;numeric&gt;

#include &lt;bobcat/semaphore&gt;

using namespace std;
using namespace FBB;

enum
{
    ROWS    = 4,
    COLS    = 6,
    COMMON  = 5,

    WORKERS = 8,
};

double lhs[ROWS][COMMON];       // Multiply lhs by rhs: consider rhsT 
double rhsT[COLS][COMMON];      // transposed: its rows are rhs's columns

struct RC
{
    size_t row;
    size_t col;
};

queue&lt;RC&gt; taskData;             // {ROWS, *} indicates end
mutex qMutex;

Semaphore producer{WORKERS};    // semantics: using 'producer' as the name
Semaphore worker{0};            // for the producer semaphore, telling the
                                // producer how many workers are available,
                                // and 'worker' for the workers semaphore, as
                                // this results in `producer.wait' in the
                                // producer, and 'worker.wait' in the worker
                                // functions. 

                                
double ip(RC spec)              // function executed by the P-tasks.
{                               // as rhsT is transposed, inner_product can be
                                //  used 
    return inner_product(lhs[spec.row], lhs[spec.row] + 5, rhsT[spec.col], 0);
};

                                // now that the function is known, define a
                                // matching typedef for the P-task. Function
                                // parameters are named for documentation
typedef packaged_task&lt; double (RC) &gt; PTask;

PTask pTask[ROWS][COLS];        // These P-tasks produce the results

                                // defining a more readable thread-id, used to
size_t counter = 0;             // show the thread computing an inner product. 
thread_local size_t threadID = ++counter; 
mutex outMutex;

RC getSpecs()
{
    lock_guard&lt;mutex&gt; lg(qMutex);

    RC ret = taskData.front();  // see what's in store
    if (ret.row == ROWS)        // ROWS: then done
        worker.notify_all();    // inform the other workers as well
    else
        taskData.pop();         // or use this request.

    return ret;
}

void workerFun()
{
    while (true)
    {
        worker.wait();          // wait for the task specs

        RC spec = getSpecs();   // get the specs

        if (spec.row == ROWS)   // all done.
            return;

// uncomment to see the thread performing this computation
//        outMutex.lock();
//        cout &lt;&lt; threadID &lt;&lt; ": " &lt;&lt; spec.row &lt;&lt; ", " &lt;&lt; spec.col &lt;&lt; '\n';
//        outMutex.unlock();
                                // otherwise perform the task
        pTask[spec.row][spec.col](spec);

        producer.notify();      // ready for the next task.
    }
}

void push(RC &amp;&amp;rc)
{
    qMutex.lock();              // push the task specs
    taskData.push( );
    qMutex.unlock();
    
    worker.notify_all();        // allow a worker to process it.
}

void produce()
{
                                // perform the matrix multitplication
    for (size_t row = 0; row != ROWS; ++row)
    {
        for (size_t col = 0; col != COLS; ++col)
        {
                                // create the matching P-task.
            pTask[row][col] = PTask{ip}; 

            producer.wait();    // wait for available worker

                                // push the task spec
            push( RC{ row, col } );
        }
    }
                          
    push( RC{ ROWS, 0 } );      // all specs were pushed: `end of tasks'
}

int main()
{
                                // some silly initialization of the matrices
    fill_n(&amp;lhs[0][0],  ROWS * COMMON, 1.);
    fill_n(&amp;rhsT[0][0], COLS * COMMON, 2.);

                                //  start the threads.
    for (size_t count = 0; count != WORKERS; ++count)
        thread{ workerFun }.detach();

    produce();                  // produce the results
        
                                // show the results.
    for (size_t row = 0; row != ROWS; ++row)
    {
        for (size_t col = 0; col != COLS; ++col)
            cout &lt;&lt; pTask[row][col].get_future().get() &lt;&lt; ' ';
        cout &lt;&lt; '\n';
    }
}






</pre>

<p>
<br><br>
    
</p><p>
</p><hr>
        <strong>Exercise 60.</strong>
        <br>
        Purpose of this exercise: learn to implement a multi-threaded algorithm (2)
<p>
The simple sort generic algorithm has two parameters: two iterators defining
the range of elements to sort.
</p><p>
Design and implement a multi-threaded variant of qsort.
</p><p>
Here is quicksort's standard, recursive implementation:
        </p><pre>
    void qsort(int *beg, int *end)
    {
        if (end - beg &lt;= 1)
            return;
    
        int lhs = *beg;
        int *mid = partition(beg + 1, end, 
            [&amp;](int arg)
            {
                return arg &lt; lhs;
            }
        );
    
        swap(*beg, *(mid - 1));
    
        qsort(beg, mid);
        qsort(mid, end);
    }
        
</pre>
        
    The multithreaded variant should use <code>async</code>. The <code>int *</code> parameters
point to an array of <code>int</code> values. That's a very basic situation, which we
will soon be able to generalize. 
<p>
To call this <code>qsort</code> on an array <code>int ia[]</code> of <code>size_t iaSize</code>
elements do <code>qsort(ia, ia + iaSize).</code>
</p><p>
<br><strong>Answer:</strong><br>
    </p><pre>#include &lt;algorithm&gt;
#include &lt;iostream&gt;
#include &lt;string&gt;
#include &lt;iterator&gt;
#include &lt;iomanip&gt;
#include &lt;thread&gt;
#include &lt;future&gt;
#include &lt;chrono&gt;

using namespace std;

//int ia[] = {4, 1, 3, 5, 7, 9, 10, 2, 8, 6};

int ia[] = {
        4, 1, 3, 5, 7, 9, 10, 2, 8, 6,
        4, 1, 3, 5, 7, 9, 10, 2, 8, 6,
        4, 1, 3, 5, 7, 9, 10, 2, 8, 6,
        4, 1, 3, 5, 7, 9, 10, 2, 8, 6,
            };

void pqsort(int *beg, int *end)
{
    if (end - beg &lt;= 1)
        return;

    int lhs = *beg;
    int *mid = 
            partition(beg + 1, end, 
                [&amp;](int arg)
                {
                    return arg &lt; lhs;
                }
            );

    this_thread::sleep_for(chrono::milliseconds(100));

    swap(*beg, *(mid - 1));
        
    auto ret1 = async(launch::async, pqsort, beg, mid);
    auto ret2 = async(launch::async, pqsort, mid, end); 

    ret1.get();         // wait() also works here.
    ret2.get();
}

int main()
{
    pqsort(ia, ia + size(ia));

    for (int val: ia)
        cout &lt;&lt; setw(2) &lt;&lt; val &lt;&lt; ' ';
    cout &lt;&lt; '\n';
}







</pre>

<p>
<br><br>
    
</p><hr>
        <strong>Exercise 61.</strong>
        <br>
        (optional)<br>Purpose of this exercise: learn to prevent overpopulation.
<p>
The multithreaded qsort function has a disadvantage: it runs the risk of
producing overpopulation as the threads tend to explode exponentially: 
most threads start two additiional threads.
</p><p>
Instead of creating additional threads another approach is possible. Consider
the following raw outline of the qsort algorithm:
        </p><pre>
    qsort(begin, end)
    {
        mid = partition(begin, end);

        qsort(begin, mid);
        qsort(mid, end);
    }
        
</pre>

<p>
In the multithreaded variant the recursive calls are implemented as
threads, but they operate completely independently from each other. Our
revised implementation will not do recursion and will not start new
threads. Instead, we pre-define a number of worker threads, performing the
following tasks:
        </p><pre>
    wait for a notification that the next range to partition might be 
                                                                available
    if it isn't, then done: notify the other threads and stop

    it it is available:
        retrieve its  begin and end
        partition it: mid = partition(begin, end);
        if the range begin..mid contains multiple elements (i.e., mid - begin
            &gt; 1) then put begin, mid on the queue containing ranges waiting to
            be processed  and notifiy other threads
        if the range mid..end contains multiple elements (i.e., end - mid
            &gt; 1) then put mid, end on the queue containing ranges waiting to
            be processed and notifiy other threads
        
</pre>

    Initially the queue is filled with begin, end, so all elements must be
sorted, and the semaphore <code>nextRange</code> is initialized with the value 1.
<p>
The main problem with the above description is that the algorithm doesn't
know when to stop. To handle this:
        </p><ul>
        <li> You cannot just check for an empty queue: once the first thread
removes the queue's front element the queue is empty, and the next thread
might conclude that it should stop. 
        </li><li> Once all threads are waiting (or are about to wait), <em>and</em>
the queue is empty then send a notification to all threads waiting on
<code>nextRange</code>.
        </li><li> The thread that receives the notification notices that the queue
is empty: it should end its task and send a notification to all threads
waiting on <code>nextRange</code>, so the other threads also can finish their work.
        </li></ul>
<p>
Implement this version of qsort using a main thread and three additional
sorting threads. Assuming an <code>int ia[]</code> array and its number of elements (in
<code>iaSize</code> are available, your <code>main</code> function should look like this:
        </p><pre>
    int main()
    {
        nextRange.push(Pair{ia, ia + iaSize});  // initialize the queue
    
        thread t1(tsort);                       // start the threads
        thread t2(tsort);
        thread t3(tsort);
    
        t1.join();                              // clean up
        t2.join();       
        t3.join();       
    
        for (int val: ia)                       // show the sorted array
            cout &lt;&lt; val &lt;&lt; ' ';
        cout &lt;&lt; '\n';
    }
        
</pre>

<p>
Our implementation defines a function <code>tsort</code>, calling a partition
function <code>int *partition(Pair pair)</code>, and <code>Pair</code> is defined as a
<code>struct</code> defining a default constructor and a construcor accepting two
<code>int *</code> arguments (I could have used a <code>std::pair</code>, but I don't like its
'first', 'second' field names).
        </p><pre>
    struct Pair
    {
        int *beg;
        int *end;
        Pair() = default;
        Pair(int *b, int *e)
        :
            beg(b),
            end(e)
        {}
    };
        
</pre>

<p>
<br><strong>Answer:</strong><br>
    Our implementation can be downloaded
<a href="https://www.icce.rug.nl/edu/2/multithreading2/answers/qsortthreadclean.cc">here</a>. 
</p><p>
The implementation is fairly basic, but should show the approach we
followed. In the implementation no class was defined, but that can easily be
done. Also, all the used functions were implemented in one source file.
</p><p>
There really is no good excuse for doing that. Our rather lame excuse is
that I (Frank) ran out of time. But it's a nice (and easy) exercise to fix the
implementation and create a more beautiful implementation.
</p><p>
<br><br>
    
</p><p>
</p><hr>
        <strong>Exercise 62.</strong>
        <br>
        Purpose of this program: learn to inspect one or more <code>futures</code> from inside
a repeat-statement, even if the future has not yet been made ready.
<p>
Consider the situation where you have multiple threads. Each of these threads
may do their job, but once a thread has completed, it must pass information to
the program, and the program must end (thus automatically ending the running
threads).
</p><p>
While the threads run, the main program may be doing something else. So, a
prototypical thread function might look like this (assuming <code>namespace
std</code>):
        </p><pre>
    string threadFun()
    {
        // do something which takes some time (maybe iteratively)
        // At some point some error may be observed,
        // and the details about the error are returned
        // (here a std::string is used, but the return type could
        // of course also be a struct)
        // so at some point this function does:
        return "some message";
    }
        
</pre>

<p>
A simple variant of the thread function could be:
        </p><pre>
    string threadFun()
    {
        cerr &lt;&lt; "entry\n";
    
        this_thread::sleep_for(chrono::seconds(5));
        cerr &lt;&lt; "first cerr\n";
    
        this_thread::sleep_for(chrono::seconds(5));
        cerr &lt;&lt; "second cerr\n";
    
        return "end the program";
    }
        
</pre>

<p>
The main function could inspect the values returned by the threads, but as it
also has to do something useful while the threads are busy it would be nice if
main could do its thing, and then briefly check whether a thread has returned
information requiring the program to stop.
</p><p>
Alternatively, the program could do its thing in a separate thread, but that
occupies a thread, which could also be used for the jobs we prefer to do in
sparate threads. If inspecting whether these latter threads have messages
hardly takes any time, then a setup like the following might be preferred:
        </p><pre>
    int main()
    {
        // start all threads

        while (true)
        {
            // do the main-task

            // inspect whether a thread indicates
            // to end the program. If so, end it.
        }
    }
        
</pre>

<p>
In this exercise only one thread needs to be executed, running
<code>threadFun</code>. 
</p><p>
The main task could be a simple:
        </p><pre>
    this_thread::sleep_for(chrono::seconds(1));
    cerr &lt;&lt; "inspecting: " &lt;&lt; ++count &lt;&lt; '\n';
        
</pre>

    (assuming <code>size_t count = 0</code>).
<p>
</p><ul>
    <li> Implement and submit <code>main</code>; convince yourself that the program's
output is:
        <pre>
    entry
    inspecting: 1
    inspecting: 2
    inspecting: 3
    inspecting: 4
    first cerr
    inspecting: 5
    inspecting: 6
    inspecting: 7
    inspecting: 8
    inspecting: 9
    second cerr
    inspecting: 10
    done
        
</pre>

    </li><li> Describe the required modifications if you run multiple threads.
    </li></ul>
<p>
The amount of code is small: submitting one source file is OK.
</p><p>
<br><strong>Answer:</strong><br>
    </p><ul>
    <li> Here is our implementation of the program. Our implementation uses
the fact that waiting for 0 time returns <code>future_status::ready</code> once
<code>threadFun</code> returns.
<p>
</p><pre>#include &lt;thread&gt;
#include &lt;iostream&gt;
#include &lt;string&gt;
#include &lt;future&gt;

using namespace std;

string threadFun()
{
    cerr &lt;&lt; "entry\n";

    this_thread::sleep_for(chrono::seconds(5));
    cerr &lt;&lt; "first cerr\n";

    this_thread::sleep_for(chrono::seconds(5));
    cerr &lt;&lt; "second cerr\n";

    return "end the program";
}


int main()
{
    size_t count = 0;

    auto fut = async(launch::async, threadFun);

    while (true)
    {
        this_thread::sleep_for(chrono::seconds(1));
        cerr &lt;&lt; "inspecting: " &lt;&lt; ++count &lt;&lt; '\n';

        if (fut.wait_for(chrono::nanoseconds(0)) == future_status::ready)
        {
            cerr &lt;&lt; "done\n";
            return 0;
        }
    }
}

    




</pre>

<p>
</p></li><li> When multiple threads are run, then store their futures in an array
or vector, and after the main task visit each of the futures and inspect their
<code>future_status</code>: if one is ready, end the program.
    </li></ul>
<p>
<br><br>
    
</p><hr>
        <strong>Exercise 63.</strong>
        <br>
        Purpose of this exerise: learn to design an elaborate multi-threaded program
<p>
Consider a situation where you want to compile multiple source files. The
source files to compile may be listed on a file, or they may be present in a
directory and possibly its subdirectories. Other ways for determining which
<strong>s</strong> to compile migh also exist.
</p><p>
How would you design a program that compiles these files concurrently, using
all the cores your computer has? If a compilation fails, then running
compilations may end and sources waiting to be compiled are not
considered. Once a compilation fails only the error-output of that compilation
must be shown. 
</p><p>
Your description should:
    </p><ul>
    <li> provide a clear, overall description of your program's flow;
    </li><li> describe the main data stuctures your program maintains;
    </li><li> describe how the various processes and access to the data structures;
        are synchronized (think about mutexes, semaphores, ending the program,
        handling error messages);
    </li><li> show a class hierarchy of the essential classes used by your program;
    </li></ul>
<p>
Your program should also offer an option allowing users to specify the number
of compilation to use. The minimum number of compilation threads must be 1: if
0 is specified, 1 is silently used. Another option specifies a 
directory where temporary files are stored. This directory may not yet exist,
and must be removed when the program ends.
</p><p>
<br><strong>Answer:</strong><br>
</p><p>
<strong>a description of the program's flow</strong>
</p><p>
The program uses a producer/consumer design. 
</p><p>
The producer requests the names of the sources to compile from a separate
object, an <code>Intaker</code>, which is an abstract base class, whose <code>next</code>
member returns the name of the next file to compile or an empty string once
all sources have been processed (classes derived from <code>Intaker</code> may decide
to skip a source file if a more recent object file already exists, but that's
an implementation detail we won't consider here)
</p><p>
The producer, as long as source files are available, enters their names in a
queue to be processed by the compiling clients. It's also possible that a
client failed to compile a source.
</p><p>
Before starting the producer as many clients were started as there are 
separate cores. Clients wait until a job becomes available, then start its
compilation in a separate child process. How the compiler is started is
determined by a program option, having a sensible default. If a compilation
fails, then the compilation's return value will show so.
</p><p>
So, there is a producer thread and one or more consumer threads. Just before
entering a new source file in the work-queue the producer should check whether
a compilation thread has indicated a a compilation failure. If so, the error
messages are shown and the program ends (with return value 1).
</p><p>
<strong>the main data stuctures of the program</strong>
</p><p>
</p><ul>
    <li> A <code>queue&lt;string&gt; jobQueue</code> holds the names of the programs to
        compile. Names are pushed onto the queue by the producer, and
        retrieved from the queue by the consumer.
    </li></ul>
<p>
<strong>synchronization</strong>
</p><p>
The producer and consumers (compiling clients) use two semaphores to
synchronize storage into and retrieval from the <code>jobQueue</code>. The initially
available number of elements in the queue is set equal to the number of
compilation threads. 
</p><p>
A mutex ensures that storage into and retrieval from the <code>jobQueue</code>
never happens simultaneously.
</p><p>
<strong>class hierarchy</strong>
</p><p>
</p><pre>
    D: Derives from; otherwise: uses and/or defines

    Classes mentioned higher up in the hierarchy may be used
    by classse mentioned lower in the hierarchy, if a line
    connecting the low and high entry exists.

    E.g., Producer uses the Intaker, but not SourceLines;
          all but Intaker, Storage and Fork have access to Options


       Arguments
          |
       Options        
          |
    +-----+------+                  Storage            
    |            |                     |
    |            +---------------------+
    |               		           |
    |    Intaker    		   +-------+--------+
    |      |  |     		   |                |
    |      |  +----------------+                |
    |      |        		   |                |
    |      D        		   |                |      Fork 
    |      |        		Producer            |       |
    +------+-------+	 	   |                +-------+   
    |              |     	   |                |
SourceLines    SourceDirs      |             Consumer
    |              |           |                |
    +--------------+-----------+----------------+
                               |
                              main
    
</pre>

<p>
<br><br>
    
</p><p>
</p><hr>
        <strong>Exercise 64.</strong>
        <br>
        (optional)<br>Purpose of this exercise:   learn to implement an elaborate multi-threaded
program (part I). 
<p>
Considering the exercise about the multi-threaded program: implement the
classes preparing the compilation jobs. So this involves finding the sources
to compile, and notifying the compiling-clients about the next available
compilation job. This part concentrates on the `Producer' section.
</p><p>
<br><br>
    
</p><hr>
        <strong>Exercise 65.</strong>
        <br>
        (optional)<br>Purpose of this exercise:   learn to implement an elaborate multi-threaded
program (part III). 
<p>
Considering the exercise about the multi-threaded program: implement the
classes actually performing a compilation. This involves calling the compiler
(using the approriate compilation flags) and determining whether the
compilation succeeeded or not. Here you also must pay attention to how error
messages that may be generated by the compiler are handled.
</p><p>
Hint: write error messages to a temporary file; upon completion the
result (<code>future</code>?) may contain information about the success of the
compilation and the name of the temporary file.
</p><p>
<br><br>
    
</p><hr>
        <strong>Exercise 66.</strong>
        <br>
        (optional)<br>Purpose of this exercise:   learn to implement an elaborate multi-threaded
program (part IV). 
<p>
Complete the multicompile program, and show its working.
</p><p>
(Earlier we developed the classes <code>Arguments</code> and <code>Storage</code>. The singleton
class <code>Options</code> wraps the <code>Arguments</code> facilities, and makes sure that the
program doesn't have to think about what to do with a particular option. E.g.,
by default maybe <code>thread::hardware_concurrency</code> is used, but this may be
altered by a program option. <code>Optiion::instance().nThreads()</code> returns the
actual number of threads that are actually used.)
</p><p>
<br><br>
    
</p><p>
</p><hr>
<ul>
    <li> <a href="https://www.icce.rug.nl/edu/2/opgaven.html">Table of Contents</a>
    </li><li> <a href="https://www.icce.rug.nl/edu/2/opgaven07.html">Previous Chapter</a>
</li></ul>
<hr>


</body></html>